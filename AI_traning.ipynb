{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "fe55883a-6887-43dd-9498-5333a51799e2",
      "cell_type": "code",
      "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ffc844bd-78bd-461f-9519-a987296e334b",
      "cell_type": "markdown",
      "source": "For training data, do some augmentation like random cropping, flipping, etc.",
      "metadata": {}
    },
    {
      "id": "5f5e7e3b-e752-4f23-ade7-78a257f68dfd",
      "cell_type": "code",
      "source": "train_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "64b4a894-b77a-4057-8eb0-6206e7efee10",
      "cell_type": "markdown",
      "source": "For test data, resize and normalize",
      "metadata": {}
    },
    {
      "id": "ae3ed4fe-2409-4c92-b3ea-515440967a33",
      "cell_type": "code",
      "source": "test_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e29e0dc0-246c-4733-9cb0-e7f247e59709",
      "cell_type": "markdown",
      "source": "Load training dataset",
      "metadata": {}
    },
    {
      "id": "8aa205ec-29d7-462f-96b6-6f1ac42f23c6",
      "cell_type": "code",
      "source": "train_dataset = datasets.ImageFolder(root='train_data_path', transform=train_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d8d6e2f4-6d06-45e3-a1f0-d388cbcbc8fe",
      "cell_type": "markdown",
      "source": "Load test dataset",
      "metadata": {}
    },
    {
      "id": "77601630-5af8-45fe-ad13-176554a35805",
      "cell_type": "code",
      "source": "test_dataset = datasets.ImageFolder(root='test_data_path', transform=test_transform)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7372f3b9-6f9a-4240-a8b6-94e2fe649fb7",
      "cell_type": "markdown",
      "source": "Define the CNN model",
      "metadata": {}
    },
    {
      "id": "6267f734-e944-4e8e-8580-7619f94ffd51",
      "cell_type": "code",
      "source": "class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n        self.relu1 = nn.ReLU()\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.relu2 = nn.ReLU()\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(32 * 56 * 56, 128)  # Adjust the input size according to your image size after pooling\n        self.relu3 = nn.ReLU()\n        self.fc2 = nn.Linear(128, num_classes)  # num_classes is the number of object categories you want to recognize\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.pool2(x)\n        x = x.view(-1, 32 * 56 * 56)\n        x = self.fc1(x)\n        x = self.relu3(x)\n        x = self.fc2(x)\n        return x",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "caf8b5eb-5f39-4c70-867e-b6e475aad53a",
      "cell_type": "markdown",
      "source": "Initialize the model loss function and optimizer",
      "metadata": {}
    },
    {
      "id": "2ed6bc5d-250d-45d8-bd75-3081633d4c2e",
      "cell_type": "code",
      "source": "model = SimpleCNN()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1ba0636-2581-4652-b5dd-d44601a29e95",
      "cell_type": "markdown",
      "source": "Training Loop",
      "metadata": {}
    },
    {
      "id": "6c145679-491a-469d-be91-7b9906994299",
      "cell_type": "code",
      "source": "num_epochs = 10\nfor epoch in range(num_epochs):\n    running_loss = 0.0\n    for i, (inputs, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if (i + 1) % 10 == 0:\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(train_loader)}], Loss: {running_loss / 10}')\n            running_loss = 0.0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5e3b0a5c-2ec8-403f-956b-2825fb600b42",
      "cell_type": "markdown",
      "source": "Evalution on the test set",
      "metadata": {}
    },
    {
      "id": "fcf27005-91f4-4afa-a432-833be25147f4",
      "cell_type": "code",
      "source": "correct = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        outputs = model(inputs)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint(f'Accuracy of the model on the test set: {100 * correct / total}%')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}